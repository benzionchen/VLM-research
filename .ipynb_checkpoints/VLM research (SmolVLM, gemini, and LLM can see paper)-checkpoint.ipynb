{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5824461f",
   "metadata": {},
   "source": [
    "Meeting with Amit 6/9 - just familiarize with the interfaces: \n",
    "\n",
    "https://huggingface.co/blog/smolvlm\n",
    "- loading the model on a gpu\n",
    "- have access to weights, gemini we dont \n",
    "\n",
    "\n",
    "Gemini api\n",
    "https://ai.google.dev/\n",
    "- gemini too big on own gpu, requires data center, cant load \n",
    "\n",
    "how to interact with the model\n",
    "\n",
    "LLMs can see and hear without any training\n",
    "\n",
    "https://arxiv.org/abs/2501.18096\n",
    "\n",
    "read intro, section 1 and section 3\n",
    "- LLMs dont have visual modality, but because they are trained on internet, there's a lot of world knowledge, and use them for tasks they weren't trained for \n",
    "- we can fine tune, but what if i don't want to do that?\n",
    "- this uses another model called CLIP, takes 2 inputs (1 image and 1 text) and match how close text is to image (like image caption matching model), it's not a generative text, it outputs a numerical score\n",
    "    - CLIP model takes in image, text => score how closely text describes image\n",
    "    - first ask llm to produce random captions (like 10), take image you want to caption, give these 10 random captions as a batch to the CLIP model, producing 10 scores per caption \n",
    "    - when you have scores, you feed back to LLM, and now picks the top scores and caption and refine that part \n",
    "    - one caption with highest score, all captions should be above this checkpoint refinement (iteratively) \n",
    "    - LLM is blind, what gives it vision is CLIP model, reasonably coherent caption for the image \n",
    "    - without fine tuning and training resources, how do we add capability to it? image, audio captioning, etc. to non-vision models\n",
    "        - and it doesnt even have to be CLIP, the scoring can be done differently using different scores, 1 idea: \n",
    "        - AI alignment (AI personalization), if you want to align AI model to preferences of users at test time \n",
    "      \n",
    "code is on github\n",
    "- don't worry about all, just care about 1 task: \"main_image_caption.py\" ~270 lines, uses most of stuff i've been using... just understand what the file is doing \n",
    "- run for loop, produce random captions, and refine them through brute force \n",
    "- \"Scorer as S\" is score, and \"Generator as G\" is the LLM\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d63c89",
   "metadata": {},
   "source": [
    "Artin suggest I read:\n",
    "\n",
    "3/28\n",
    "1. https://hao-ai-lab.github.io/blogs/distserve/\n",
    "2. https://www.anyscale.com/blog/continuous-batching-llm-inference\n",
    "3. https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md\n",
    "4. https://arxiv.org/abs/2305.13245\n",
    "\n",
    "\n",
    "6/25\n",
    "1. ROPE - https://www.youtube.com/watch?v=o29P0Kpobz0\n",
    "2. MoE - https://huggingface.co/blog/moe\n",
    "3. GQA - https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a/\n",
    "4. Spec Decoding - https://www.youtube.com/watch?v=S-8yr_RibJ4\n",
    "5. Chinchilla Scaling Laws - https://arxiv.org/pdf/2203.15556\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7652b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
