{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5824461f",
   "metadata": {},
   "source": [
    "Meeting with Amit 6/9 - just familiarize with the interfaces: \n",
    "\n",
    "https://huggingface.co/blog/smolvlm\n",
    "- loading the model on a gpu\n",
    "- have access to weights, gemini we dont \n",
    "\n",
    "\n",
    "Gemini api\n",
    "https://ai.google.dev/\n",
    "- gemini too big on own gpu, requires data center, cant load \n",
    "\n",
    "how to interact with the model\n",
    "\n",
    "LLMs can see and hear without any training\n",
    "\n",
    "https://arxiv.org/abs/2501.18096\n",
    "\n",
    "read intro, section 1 and section 3\n",
    "- LLMs dont have visual modality, but because they are trained on internet, there's a lot of world knowledge, and use them for tasks they weren't trained for \n",
    "- we can fine tune, but what if i don't want to do that?\n",
    "- this uses another model called CLIP, takes 2 inputs (1 image and 1 text) and match how close text is to image (like image caption matching model), it's not a generative text, it outputs a numerical score\n",
    "    - CLIP model takes in image, text => score how closely text describes image\n",
    "    - first ask llm to produce random captions (like 10), take image you want to caption, give these 10 random captions as a batch to the CLIP model, producing 10 scores per caption \n",
    "    - when you have scores, you feed back to LLM, and now picks the top scores and caption and refine that part \n",
    "    - one caption with highest score, all captions should be above this checkpoint refinement (iteratively) \n",
    "    - LLM is blind, what gives it vision is CLIP model, reasonably coherent caption for the image \n",
    "    - without fine tuning and training resources, how do we add capability to it? image, audio captioning, etc. to non-vision models\n",
    "        - and it doesnt even have to be CLIP, the scoring can be done differently using different scores, 1 idea: \n",
    "        - AI alignment (AI personalization), if you want to align AI model to preferences of users at test time \n",
    "      \n",
    "code is on github\n",
    "- don't worry about all, just care about 1 task: \"main_image_caption.py\" ~270 lines, uses most of stuff i've been using... just understand what the file is doing \n",
    "- run for loop, produce random captions, and refine them through brute force \n",
    "- \"Scorer as S\" is score, and \"Generator as G\" is the LLM\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d63c89",
   "metadata": {},
   "source": [
    "Artin suggest I read:\n",
    "\n",
    "3/28\n",
    "1. https://hao-ai-lab.github.io/blogs/distserve/\n",
    "2. https://www.anyscale.com/blog/continuous-batching-llm-inference\n",
    "3. https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md\n",
    "4. https://arxiv.org/abs/2305.13245\n",
    "\n",
    "\n",
    "6/25\n",
    "1. ROPE - https://www.youtube.com/watch?v=o29P0Kpobz0\n",
    "2. MoE - https://huggingface.co/blog/moe\n",
    "3. GQA - https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a/\n",
    "4. Spec Decoding - https://www.youtube.com/watch?v=S-8yr_RibJ4\n",
    "5. Chinchilla Scaling Laws - https://arxiv.org/pdf/2203.15556\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf92e1f",
   "metadata": {},
   "source": [
    "LLMs can see and hear without any training Notes\n",
    "\n",
    "https://arxiv.org/abs/2501.18096\n",
    "\n",
    "- investigates whether frozen LLMs — like GPT-4, LLaMA, etc. — can process raw audio and image data without any gradient-based training (i.e., no fine-tuning, no updates to weights)... \"Can we use a pretrained LLM, as-is, to “understand” images and audio, just by converting the input into tokens it can already handle — like language?\" \n",
    "\n",
    "CLIP = model that understands images and text together \n",
    "- way to get a list of image tokens — a kind of compressed representation of the image\n",
    "\n",
    "VQGAN\n",
    "- model that turns images into discrete patches (small tiles), and then compresses each tile into a token (an integer code), the entire image becomes a sequence of these codes\n",
    "- useful because the output is very much like what a language model already expects: a series of integer tokens\n",
    "\n",
    "Wav2Vec2 \n",
    "- model listens to raw audio (like someone talking) and breaks it into short slices of time\n",
    "- each slice is turned into a feature, and then compressed into a code, similar to how image patches are compressed\n",
    "\n",
    "EnCodec\n",
    "- sound compression model it takes in audio (like music or voice) and converts it into tokens, just like VQGAN does for images\n",
    "- audio tokens are also integers\n",
    "\n",
    "\n",
    "\n",
    "MM1-to-MM6 (multi-modal 1 -> 6 different inputs)\n",
    "- frozen pretrained encoders for audio and images to convert them into discrete tokens (like text tokens), feeding those discrete tokens directly into the frozen LLM, using only prompting (no backpropagation) to get the LLM to process and reason about the input — even if the input came from image or audio\n",
    "\n",
    "\n",
    "MM1\n",
    "- feed an image or sound to the language model as a list of codes, the way you might give it a string of numbers or words\n",
    "    - like \"< Image> 4 17 230 88 99 122 ... < /Image>\" or something and ask \"Q: What is in this image?\" \n",
    "    - the LLM treats the numbers and code just like it would treat strange words, and this works because it just tries to answer the question based on the pattern of numbers it sees \n",
    "    - the number codes from the images or sounds don't belong to LLM's dictionary of words, so it doesn't know what to do with them, so they remapped the # codes to make sure they lie inside the range that the language model \n",
    "        - ChatGPT: When language model is given unfamiliar tokens from images and sounds, it usually tries to make an intelligent guess, but does not say \"I don't know\" unless the prompt leads it to. But it does not hallucinate randomly either because the image/audio tokens are not meaningless: they come from a real encoder that captureed structure from the input\n",
    "        - (question for amit: how believable is this?)\n",
    "    - they use something like VQ-VAE, EnCodec, or Wav2Vec 2.0 — models that output discrete token IDs (not continuous embeddings), for example, VQ-VAE will turn an image into a grid of codebook indices like: [271, 23, 98, 450, ...]\n",
    "    \n",
    "   \n",
    "MM2\n",
    "- they remap the number codes to make sure they're within the range of the language model can accept\n",
    "- for example, language model uses tokens from 0 to 50 english words, we shift the image tokens to start at 51 and above to not confuse the model's word dictionary and give each input type their own space \n",
    "    - (this part to me is unclear why it's important to shift into a new range, or why this step really matters?)\n",
    "        - ChatGPT: You want to tell it about an image (which is written in \"Image Language\").So you use a separate interpreter (like CLIP or VQGAN) to translate the image into a list of made-up English words.\n",
    "        - The language model can give smart answers, even though it was never taught what these image-tokens mean. They shift the image/audio token IDs to a different range (e.g. 51,000 to 60,000) so they don’t overlap with text tokens. Example: The word “cat” might be token ID 1023.The image patch representing a cat’s ear might be token ID 51023. This keeps the two vocabularies separate.\n",
    "        - Even though the embeddings are new and untrained: The structure of the image/audio tokens is not random — they come from a pretrained encoder that has already clustered similar visual/audio concepts. The language model is very good at reasoning over new symbolic sequences, even if it doesn’t know what each token means. With enough context and a good prompt, the model generalizes based on what it sees in the token sequence.\n",
    "\n",
    "\n",
    "MM3 \n",
    "- for this experiment, they try giving the language model inputs from mroe than one source at the same time, like an image and a sound together, this will test whether the langauge model can combine different types of input and still make sense of them \n",
    "    - ex: <Image> ...tokens from image... </Image> <Audio> ...tokens from audio... </Audio>... Q: What is happening in the scene?\n",
    "\n",
    "MM4 \n",
    "- this step, the model was prompted to name objects in a photo, caption an image, describe sounds\n",
    "- pick an encoder that knows how to turn an image or sound into a list of tokens + make sure the tokens are wrapped in special markers + write a question in plain english and the model does the rest\n",
    "\n",
    "\n",
    "MM5 \n",
    "- compare their frozen language model to other models that were deliberately trained on both words and images together\n",
    "- even though other models had training on multimodal data, frozen language model performs almost as well (reasonably close)\n",
    "- means that LLMs are more flexible than we initially thought, they can reason about input formats they weren't explicitly trained on as long as the input is shaped correctly \n",
    "\n",
    "MM6\n",
    "multi-turn conversation \n",
    "\n",
    "User: <Image>...tokens...</Image> What do you see?\n",
    "LLM: A dog playing in a field\n",
    "User: What color is the dog?\n",
    "LLM: The dog is brown and white\n",
    "\n",
    "- even though the model never trained on this kind of task with image input, it can still carry on a conversation that refers back to the image\n",
    "\n",
    "conclusion:\n",
    "- LLMs can become multimodal if we plug in visual/audio encoder and clever prompting, no need to retrain from scratch\n",
    "- LLMs are fundamentally built on transformers and the architecture is pretty flexible in the sense that it doesn't really matter or care about whether a token came from a word, image patch, or audio frame, it sees a sequence of vectors \n",
    "    - though this is a little besides the point of the paper, which is to examine how LLMs can understand other modalities without explicitly trained on it\n",
    "    - the question is... the LLM doesn't care and is flexible, then why do models trained on video and images and audio perform better for respective tasks?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2a083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
